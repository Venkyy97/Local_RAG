

# Technical Assistant API with PDF Processing and Vector Store

This repository contains a Flask-based API that provides technical assistance by answering queries using an LLM model (Ollama's "llama3") and retrieving document content from uploaded PDFs using a vector store (Chroma) with embeddings. It supports text-based and PDF-based queries.

## Setup

### Requirements
- Python 3.8+
- Libraries:
  - Flask
  - LangChain (for LLM and vector store integration)
  - PDFPlumber (for PDF processing)
  - Chroma (for document storage)

Install dependencies:
```bash
pip install -r requirements.txt
```

### Folder Structure
- **`db/`**: Storage for vector data for faster query processing.
- **`pdf/`**: Stores uploaded PDF files.

## API Endpoints

### 1. `/ai` (POST)
Handles a general AI query and returns the response generated by the LLM.
- **Request**: `{"query": "your question here"}`
- **Response**: `{"answer": "LLM-generated answer"}`

### 2. `/ask_pdf` (POST)
Handles PDF-based queries using vectorized embeddings from the Chroma vector store.
- **Request**: `{"query": "your PDF-related question"}`
- **Response**: `{"answer": "LLM answer", "sources": [{"source": "source_name", "page_content": "content excerpt"}]}`

### 3. `/pdf` (POST)
Uploads and processes a PDF file, splits it into chunks, and stores embeddings for fast retrieval.
- **Request**: Multipart form-data with `file`.
- **Response**: `{"status": "Successfully Uploaded", "filename": "filename", "doc_len": total_docs, "chunks": total_chunks}`

## Code Overview

- **Model and Embedding Setup**: `Ollama` is used as the LLM model, and `FastEmbedEmbeddings` generates document embeddings.
- **Text Splitter**: `RecursiveCharacterTextSplitter` divides text into chunks of `chunk_size=1024` with an overlap of 80.
- **Prompt Template**: Uses a template that helps generate responses for technical document searches.
- **Vector Store**: `Chroma` is set up to store and retrieve document embeddings based on similarity score.
  
## Running the App
Start the server:
```bash
python app.py
```

The API will be accessible at `http://0.0.0.0:8080`.


